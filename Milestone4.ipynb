{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES/TODO:\n",
    "1. How do we deal with standardized scores in model evaluation?\n",
    "2. Which criterion do we use for evaluating our classifier? (Quadratic weighted kappa or spearman's correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import util\n",
    "# TODO if needed, include the words and stopwords imports\n",
    "# HOWEVER, to use them, you will need to download nltk stuff first if not done already\n",
    "# To do so, open a python shell (i.e. go to terminal and enter python), and then type\n",
    "#\n",
    "# import nltk\n",
    "# nltk.download()\n",
    "\n",
    "# After this, select the words and stopwords corpuses, and download them\n",
    "\n",
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.corpus import words\n",
    "\n",
    "# Regular expressions might be useful\n",
    "import re\n",
    "\n",
    "# Beautiful soup might be useful\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# for modeling\n",
    "from sklearn.linear_model import LogisticRegression as LogReg\n",
    "from sklearn.linear_model import LogisticRegressionCV as LogRegCV\n",
    "# from sklearn.cross_validation import cross_val_predict \n",
    "# from sklearn import cross_validation\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def append_regularized_scores(old_df):\n",
    "    new_df = old_df.copy()\n",
    "    new_df['std_score'] = new_df.groupby(['essay_set'])[['score']].apply(lambda x: (x - np.mean(x)) / (np.std(x)))\n",
    "    return new_df\n",
    "\n",
    "def create_regularization_data(old_df):\n",
    "    #getting the number of datasets\n",
    "    max_essay_set = max(old_df['essay_set'])\n",
    "    #list of the regularized values\n",
    "    regularization_data = []\n",
    "    for i in range(max_essay_set+1):\n",
    "        mean = np.mean((old_df[old_df['essay_set'] == i + 1])['score'])\n",
    "        std = np.std((old_df[old_df['essay_set'] == i + 1])['score'])\n",
    "        regularization_data.append([i + 1, mean, std])\n",
    "    return regularization_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regularized data for each essay set =  [[1, 8.528323051037576, 1.5381336495587767], [2, 6.749444444444444, 1.3844371990179603], [3, 1.8482039397450754, 0.8149207612821795], [4, 1.4322033898305084, 0.9395167668768533], [5, 2.4088642659279778, 0.9705520523317599], [6, 2.72, 0.970360757656664], [7, 16.062460165710643, 4.583888354164165], [8, 36.95020746887967, 5.749521294509325], [9, nan, nan]]\n",
      "\n",
      "\n",
      "   essay_id  essay_set                                              essay  \\\n",
      "0         1          1  Dear local newspaper, I think effects computer...   \n",
      "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
      "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
      "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
      "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
      "\n",
      "   score  std_score  \n",
      "0      8  -0.343483  \n",
      "1      9   0.306655  \n",
      "2      7  -0.993622  \n",
      "3     10   0.956794  \n",
      "4      8  -0.343483  \n",
      "\n",
      "\n",
      "mean and standard deviation of essay set 1 =  5.145133400155731e-16 , 1.0000000000000064\n",
      "mean and standard deviation of essay set 2 =  1.8861455607242937e-16 , 1.0000000000000007\n",
      "mean and standard deviation of essay set 3 =  -8.542156296073047e-17 , 0.9999999999999976\n",
      "mean and standard deviation of essay set 4 =  -1.3303858956101453e-16 , 1.0000000000000004\n",
      "mean and standard deviation of essay set 5 =  1.1314433539046957e-16 , 0.999999999999986\n",
      "mean and standard deviation of essay set 6 =  -5.913787977836668e-16 , 0.9999999999999828\n",
      "mean and standard deviation of essay set 7 =  1.3200261647152516e-16 , 1.0000000000000007\n",
      "mean and standard deviation of essay set 8 =  -2.549059779913914e-17 , 1.0000000000000004\n"
     ]
    }
   ],
   "source": [
    "# Read in training data\n",
    "# Note that for essay set 2, score becomes average of 2 domain scores\n",
    "train_cols = ['essay_id', 'essay_set', 'essay', 'domain1_score', 'domain2_score']\n",
    "train_df = pd.read_csv('../../data/training_set_rel3.tsv', delimiter='\\t', usecols=train_cols,dtype={'essay_set':int},encoding = \"ISO-8859-1\")\n",
    "for i in range(train_df.shape[0]):\n",
    "    if not np.isnan(train_df.get_value(i, 'domain2_score')):\n",
    "        assert (train_df.get_value(i, 'essay_set') == 2)\n",
    "        new_val = train_df.get_value(i, 'domain1_score') + train_df.get_value(i, 'domain2_score')\n",
    "        train_df.set_value(i, 'domain1_score', new_val) \n",
    "train_df = train_df.drop('domain2_score', axis=1)\n",
    "train_df = train_df.rename(columns={'domain1_score': 'score'})\n",
    "\n",
    "################\n",
    "regularization_data = create_regularization_data(train_df)\n",
    "train_df = append_regularized_scores(train_df)\n",
    "\n",
    "print (\"The regularized data for each essay set = \", regularization_data)\n",
    "print (\"\\n\")\n",
    "\n",
    "#print train_df[train_df['essay_set'] == 2].head()\n",
    "print (train_df.head())\n",
    "print (\"\\n\")\n",
    "\n",
    "#validate that the standardization works\n",
    "max_essay_set = max(train_df['essay_set'])\n",
    "for i in range (max_essay_set):\n",
    "    valid = train_df[train_df[\"essay_set\"] == i + 1][\"std_score\"]\n",
    "    print (\"mean and standard deviation of essay set \" + str(i + 1) + \" = \", np.mean(valid), \",\", np.std(valid))\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing training data!\n"
     ]
    }
   ],
   "source": [
    "# Show nothing is empty in training set\n",
    "if train_df.isnull().any().any():\n",
    "    print ('Training data is missing!')\n",
    "else:\n",
    "    print ('No missing training data!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      prediction_id  essay_id  essay_set  essay_weight  predicted_score\n",
      "0              1788      1788          1           1.0                7\n",
      "1              1789      1789          1           1.0                8\n",
      "2              1790      1790          1           1.0                9\n",
      "3              1791      1791          1           1.0                9\n",
      "4              1792      1792          1           1.0                9\n",
      "5              1793      1793          1           1.0                9\n",
      "6              1794      1794          1           1.0                9\n",
      "7              1795      1795          1           1.0               11\n",
      "8              1796      1796          1           1.0                8\n",
      "9              1797      1797          1           1.0               10\n",
      "10             1798      1798          1           1.0                8\n",
      "11             1799      1799          1           1.0                9\n",
      "12             1800      1800          1           1.0                8\n",
      "13             1801      1801          1           1.0               12\n",
      "14             1802      1802          1           1.0                7\n",
      "15             1803      1803          1           1.0                8\n",
      "16             1804      1804          1           1.0                8\n",
      "17             1805      1805          1           1.0                9\n",
      "18             1806      1806          1           1.0                8\n",
      "19             1807      1807          1           1.0                8\n",
      "20             1808      1808          1           1.0                9\n",
      "21             1809      1809          1           1.0                9\n",
      "22             1810      1810          1           1.0               11\n",
      "23             1811      1811          1           1.0               10\n",
      "24             1812      1812          1           1.0                9\n",
      "25             1814      1814          1           1.0                8\n",
      "26             1815      1815          1           1.0                8\n",
      "27             1816      1816          1           1.0                8\n",
      "28             1817      1817          1           1.0                9\n",
      "29             1818      1818          1           1.0                8\n",
      "...             ...       ...        ...           ...              ...\n",
      "4788          24902     21902          8           1.0               40\n",
      "4789          24903     21903          8           1.0               41\n",
      "4790          24904     21904          8           1.0               32\n",
      "4791          24905     21905          8           1.0               41\n",
      "4792          24906     21906          8           1.0               38\n",
      "4793          24908     21908          8           1.0               36\n",
      "4794          24910     21910          8           1.0               44\n",
      "4795          24911     21911          8           1.0               23\n",
      "4796          24912     21912          8           1.0               37\n",
      "4797          24913     21913          8           1.0               39\n",
      "4798          24915     21915          8           1.0               30\n",
      "4799          24916     21916          8           1.0               36\n",
      "4800          24917     21917          8           1.0               27\n",
      "4801          24918     21918          8           1.0               35\n",
      "4802          24919     21919          8           1.0               37\n",
      "4803          24921     21921          8           1.0               39\n",
      "4804          24922     21922          8           1.0               41\n",
      "4805          24923     21923          8           1.0               41\n",
      "4806          24924     21924          8           1.0               39\n",
      "4807          24925     21925          8           1.0               37\n",
      "4808          24926     21926          8           1.0               36\n",
      "4809          24927     21927          8           1.0               35\n",
      "4810          24928     21928          8           1.0               37\n",
      "4811          24929     21929          8           1.0               38\n",
      "4812          24932     21932          8           1.0               33\n",
      "4813          24933     21933          8           1.0               33\n",
      "4814          24934     21934          8           1.0               35\n",
      "4815          24935     21935          8           1.0               38\n",
      "4816          24937     21937          8           1.0               32\n",
      "4817          24938     21938          8           1.0               39\n",
      "\n",
      "[4818 rows x 5 columns]\n",
      "   essay_id  essay_set                                              essay  \\\n",
      "0      1788          1  Dear @ORGANIZATION1, @CAPS1 more and more peop...   \n",
      "1      1789          1  Dear @LOCATION1 Time @CAPS1 me tell you what I...   \n",
      "2      1790          1  Dear Local newspaper, Have you been spending a...   \n",
      "3      1791          1  Dear Readers, @CAPS1 you imagine how life woul...   \n",
      "4      1792          1  Dear newspaper, I strongly believe that comput...   \n",
      "\n",
      "   score  \n",
      "0      7  \n",
      "1      8  \n",
      "2      9  \n",
      "3      9  \n",
      "4      9  \n"
     ]
    }
   ],
   "source": [
    "# Read in validation data\n",
    "valid_cols = ['essay_id', 'essay_set', 'essay', 'domain1_predictionid', 'domain2_predictionid']\n",
    "valid_df = pd.read_csv('../../data/valid_set.tsv', delimiter='\\t', usecols=valid_cols,dtype={'essay_set':int},encoding='ISO-8859-1')\n",
    "valid_df['score'] = pd.Series([0] * valid_df.shape[0], index=valid_df.index)\n",
    "\n",
    "# scores are stored in separate data set, we'll put them in same one\n",
    "valid_scores = pd.read_csv('../../data/valid_sample_submission_5_column.csv',delimiter=',',encoding='ISO-8859-1')\n",
    "print(valid_scores)\n",
    "# put each score in our data set, and make sure to handle essay set 2\n",
    "for i in range(valid_df.shape[0]):\n",
    "    dom1_predid = valid_df.get_value(i, 'domain1_predictionid')\n",
    "    row = valid_scores[valid_scores['prediction_id'] == dom1_predid]\n",
    "    score = row.get_value(row.index[0], 'predicted_score')\n",
    "    \n",
    "    dom2_predid = valid_df.get_value(i, 'domain2_predictionid')\n",
    "    if not np.isnan(dom2_predid):\n",
    "        assert valid_df.get_value(i, 'essay_set') == 2\n",
    "        rowB = valid_scores[valid_scores['prediction_id'] == dom2_predid]\n",
    "        scoreB = rowB.get_value(rowB.index[0], 'predicted_score')\n",
    "        score += scoreB\n",
    "        \n",
    "    valid_df.set_value(i, 'score', score)\n",
    "        \n",
    "valid_df = valid_df.drop(['domain1_predictionid', 'domain2_predictionid'], axis=1)\n",
    "#print valid_df[valid_df['essay_set'] == 2].head()\n",
    "print (valid_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#####################################\n",
    "#COUNTING THE NUMBER OF UNIQUE WORDS#\n",
    "#####################################\n",
    "\n",
    "def fill_unique_words_column(train_df, valid_df):\n",
    "\n",
    "    #percentage of unique words to the total number of words\n",
    "    unique_word_percentages_train = []\n",
    "    unique_word_percentages_valid = []\n",
    "\n",
    "    for i in range(len(train_df)):\n",
    "        splits = train_df.iloc[i][\"essay\"].split()\n",
    "        total_words = len(splits)\n",
    "        unique_words = len(Counter(splits))\n",
    "        percentage = float(unique_words) / total_words\n",
    "        unique_word_percentages_train.append(percentage)\n",
    "        \n",
    "    for i in range(len(valid_df)):\n",
    "        splits = valid_df.iloc[i][\"essay\"].split()\n",
    "        total_words = len(splits)\n",
    "        unique_words = len(Counter(splits))\n",
    "        percentage = float(unique_words) / total_words\n",
    "        unique_word_percentages_valid.append(percentage)    \n",
    "\n",
    "    #Add the features to the dataset\n",
    "    train_df[\"unique_words\"] = unique_word_percentages_train\n",
    "    valid_df[\"unique_words\"] = unique_word_percentages_valid\n",
    "\n",
    "    #train_df, valid_df = util.append_standardized_column(train_df, valid_df, 'unique_words')\n",
    "\n",
    "    return train_df, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       essay_id  essay_set                                              essay  \\\n",
       " 0             1          1  Dear local newspaper, I think effects computer...   \n",
       " 1             2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       " 2             3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       " 3             4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       " 4             5          1  Dear @LOCATION1, I know having computers has a...   \n",
       " 5             6          1  Dear @LOCATION1, I think that computers have a...   \n",
       " 6             7          1  Did you know that more and more people these d...   \n",
       " 7             8          1  @PERCENT1 of people agree that computers make ...   \n",
       " 8             9          1  Dear reader, @ORGANIZATION1 has had a dramatic...   \n",
       " 9            10          1  In the @LOCATION1 we have the technology of a ...   \n",
       " 10           11          1  Dear @LOCATION1, @CAPS1 people acknowledge the...   \n",
       " 11           12          1  Dear @CAPS1 @CAPS2 I feel that computers do ta...   \n",
       " 12           13          1  Dear local newspaper I raed ur argument on the...   \n",
       " 13           14          1  My three detaileds for this news paper article...   \n",
       " 14           15          1  Dear, In this world today we should have every...   \n",
       " 15           16          1  Dear @ORGANIZATION1, The computer blinked to l...   \n",
       " 16           17          1  Dear Local Newspaper, I belive that computers ...   \n",
       " 17           18          1  Dear Local Newspaper, I must admit that the ex...   \n",
       " 18           19          1  I aegre waf the evansmant ov tnachnolage. The ...   \n",
       " 19           20          1  Well computers can be a good or a bad thing. I...   \n",
       " 20           21          1  Dear @CAPS1 of the @CAPS2 @CAPS3 daily, I am w...   \n",
       " 21           22          1  Dear local Newspaper @CAPS1 a take all your co...   \n",
       " 22           23          1  Dear local newspaper, @CAPS1 you ever see a ch...   \n",
       " 23           24          1  Dear local newspaper, I've heard that not many...   \n",
       " 24           25          1  Dear @CAPS1, @CAPS2 off, I beileve that comput...   \n",
       " 25           26          1  Do you think that computers are useless? Or do...   \n",
       " 26           27          1  Computers a good because you can get infermati...   \n",
       " 27           28          1  Dear Newspaper, Computers are high tec and hav...   \n",
       " 28           29          1  Dear local newspaper, @CAPS1 people throughout...   \n",
       " 29           30          1  Dear Newspaper People, I think that computers ...   \n",
       " ...         ...        ...                                                ...   \n",
       " 12946     21592          8   We all understand the benefits of laughter. L...   \n",
       " 12947     21594          8        It was midsummer, and i could feel the c...   \n",
       " 12948     21595          8   Have you ever experienced a time with your fr...   \n",
       " 12949     21596          8   I woke up just like any other day happy yet l...   \n",
       " 12950     21598          8   Laughter is an important part of my life, eit...   \n",
       " 12951     21599          8   I sat at the table, speechless, as they told ...   \n",
       " 12952     21601          8   As I remember back, it was @DATE1. It was a h...   \n",
       " 12953     21603          8   Those eyes, it was like I was looking out int...   \n",
       " 12954     21604          8  Some say that laugh is the common language bet...   \n",
       " 12955     21605          8   Laughter is an integral element to many situa...   \n",
       " 12956     21606          8  One time I was at my friend @PERSON1's house, ...   \n",
       " 12957     21607          8   LAUGHTER @CAPS1 knows that laughter is a heal...   \n",
       " 12958     21608          8  One thing that people in the world love to do ...   \n",
       " 12959     21609          8   Laughter, to me, is an important aspect of my...   \n",
       " 12960     21610          8   People always say that the worst parts of lif...   \n",
       " 12961     21611          8   Why is it that people can look back at someth...   \n",
       " 12962     21613          8   Before my best friend moved away, we would st...   \n",
       " 12963     21615          8                                @ORGANIZATION1  ...   \n",
       " 12964     21617          8   Morose and somnolent, I woke up. I woke up to...   \n",
       " 12965     21618          8   A while back my mom had decided to send me to...   \n",
       " 12966     21619          8                              I dont like computers   \n",
       " 12967     21620          8   Everyone knows how important a laugh can be. ...   \n",
       " 12968     21621          8   Laughter is an important part of my family. W...   \n",
       " 12969     21623          8   laughter is an important part of any kind of ...   \n",
       " 12970     21624          8  Sometime ago on a hot @DATE1 day my @NUM1 ,@PE...   \n",
       " 12971     21626          8   In most stories mothers and daughters are eit...   \n",
       " 12972     21628          8   I never understood the meaning laughter is th...   \n",
       " 12973     21629          8  When you laugh, is @CAPS5 out of habit, or is ...   \n",
       " 12974     21630          8                                 Trippin' on fen...   \n",
       " 12975     21633          8   Many people believe that laughter can improve...   \n",
       " \n",
       "        score  std_score  unique_words  \n",
       " 0          8  -0.343483      0.544379  \n",
       " 1          9   0.306655      0.515513  \n",
       " 2          7  -0.993622      0.598566  \n",
       " 3         10   0.956794      0.524809  \n",
       " 4          8  -0.343483      0.486022  \n",
       " 5          8  -0.343483      0.565041  \n",
       " 6         10   0.956794      0.519038  \n",
       " 7         10   0.956794      0.504149  \n",
       " 8          9   0.306655      0.550790  \n",
       " 9          9   0.306655      0.476096  \n",
       " 10         8  -0.343483      0.664615  \n",
       " 11         8  -0.343483      0.436224  \n",
       " 12         7  -0.993622      0.607843  \n",
       " 13         6  -1.643760      0.469055  \n",
       " 14         6  -1.643760      0.613636  \n",
       " 15        12   2.257071      0.535985  \n",
       " 16         8  -0.343483      0.489552  \n",
       " 17         8  -0.343483      0.513661  \n",
       " 18         4  -2.944037      0.803030  \n",
       " 19         6  -1.643760      0.628205  \n",
       " 20         8  -0.343483      0.544959  \n",
       " 21         3  -3.594176      0.696429  \n",
       " 22        10   0.956794      0.477927  \n",
       " 23        11   1.606932      0.525939  \n",
       " 24         8  -0.343483      0.474403  \n",
       " 25         9   0.306655      0.538889  \n",
       " 26         4  -2.944037      0.641667  \n",
       " 27         9   0.306655      0.591667  \n",
       " 28         9   0.306655      0.514825  \n",
       " 29         8  -0.343483      0.513619  \n",
       " ...      ...        ...           ...  \n",
       " 12946     40   0.530443      0.535865  \n",
       " 12947     32  -0.860977      0.484034  \n",
       " 12948     36  -0.165267      0.571429  \n",
       " 12949     31  -1.034905      0.509749  \n",
       " 12950     30  -1.208832      0.521858  \n",
       " 12951     47   1.747936      0.458186  \n",
       " 12952     40   0.530443      0.494145  \n",
       " 12953     35  -0.339195      0.554852  \n",
       " 12954     33  -0.687050      0.510121  \n",
       " 12955     36  -0.165267      0.608553  \n",
       " 12956     36  -0.165267      0.490698  \n",
       " 12957     48   1.921863      0.488998  \n",
       " 12958     40   0.530443      0.414105  \n",
       " 12959     40   0.530443      0.519578  \n",
       " 12960     40   0.530443      0.486455  \n",
       " 12961     42   0.878298      0.498229  \n",
       " 12962     40   0.530443      0.503979  \n",
       " 12963     32  -0.860977      0.394256  \n",
       " 12964     36  -0.165267      0.454930  \n",
       " 12965     40   0.530443      0.376569  \n",
       " 12966     10  -4.687383      1.000000  \n",
       " 12967     33  -0.687050      0.425127  \n",
       " 12968     44   1.226153      0.434053  \n",
       " 12969     35  -0.339195      0.438482  \n",
       " 12970     30  -1.208832      0.399756  \n",
       " 12971     35  -0.339195      0.450888  \n",
       " 12972     32  -0.860977      0.478022  \n",
       " 12973     40   0.530443      0.504284  \n",
       " 12974     40   0.530443      0.517794  \n",
       " 12975     40   0.530443      0.539615  \n",
       " \n",
       " [12976 rows x 6 columns],\n",
       "       essay_id  essay_set                                              essay  \\\n",
       " 0         1788          1  Dear @ORGANIZATION1, @CAPS1 more and more peop...   \n",
       " 1         1789          1  Dear @LOCATION1 Time @CAPS1 me tell you what I...   \n",
       " 2         1790          1  Dear Local newspaper, Have you been spending a...   \n",
       " 3         1791          1  Dear Readers, @CAPS1 you imagine how life woul...   \n",
       " 4         1792          1  Dear newspaper, I strongly believe that comput...   \n",
       " 5         1793          1  Dear local newspaper, @CAPS1 the caveman found...   \n",
       " 6         1794          1  Dear newspaper editor, @CAPS1 now is a letter ...   \n",
       " 7         1795          1  Dear @ORGANIZATION1, @CAPS1, there has been so...   \n",
       " 8         1796          1  Dear Local Newspaper, I would like to complain...   \n",
       " 9         1797          1  Dear Newspaper, @CAPS1 having kids wasting the...   \n",
       " 10        1798          1  Dear @CAPS1 @CAPS2, @CAPS3 should think that c...   \n",
       " 11        1799          1  Dear Local @CAPS1, Computers, and other techno...   \n",
       " 12        1800          1  Why people should use computers? Computers are...   \n",
       " 13        1801          1  @CAPS1, its time for supper! @CAPS2 on, mom I ...   \n",
       " 14        1802          1  Dear local newspaper: I am writing this letter...   \n",
       " 15        1803          1  Effects of computers @CAPS1 local newspaper, H...   \n",
       " 16        1804          1  Dear to whom it @MONTH1 concern, @CAPS1 is an ...   \n",
       " 17        1805          1  Dear @ORGANIZATION1, Advances in technology ca...   \n",
       " 18        1806          1  Dear newspaper @CAPS1, I am writing this lette...   \n",
       " 19        1807          1  Dear Local newspaper, I do agree that to many ...   \n",
       " 20        1808          1  Dear Local Newspaper, I heard about people's o...   \n",
       " 21        1809          1  Dear @CAPS1, I believe that computers have an ...   \n",
       " 22        1810          1  Although some people believe computers aren't ...   \n",
       " 23        1811          1  Computors are looked at as either friend or fo...   \n",
       " 24        1812          1  Dear local Newspaper, I am writing to you toda...   \n",
       " 25        1814          1  Dear news paper, Computers are good for people...   \n",
       " 26        1815          1  Dear @CAPS1 @CAPS2 Newspaper, Have you ever wa...   \n",
       " 27        1816          1  Dear Newspaper. I @CAPS2 that people are spend...   \n",
       " 28        1817          1  Dear @ORGANIZATION1, Computers have bad effect...   \n",
       " 29        1818          1  Dear @CAPS1, @CAPS2 name is @CAPS3 @CAPS4. I g...   \n",
       " ...        ...        ...                                                ...   \n",
       " 4188     21902          8  Just @CAPS1 @CAPS2 @DATE1 is @LOCATION2's @CAP...   \n",
       " 4189     21903          8  Dad's @CAPS1 In the not so distant past I need...   \n",
       " 4190     21904          8   It's that amazing feeling in your core, where...   \n",
       " 4191     21905          8   @PERSON1 sits at a table, working on an art p...   \n",
       " 4192     21906          8     Have you ever been furious enough to cry ? ...   \n",
       " 4193     21908          8   Once when I was on my way over to my best fri...   \n",
       " 4194     21910          8   My mom and I have a unique relationship. She ...   \n",
       " 4195     21911          8   This test is a bunch of crap and its funny th...   \n",
       " 4196     21912          8   Working at the local glass factory is not nec...   \n",
       " 4197     21913          8  The @ORGANIZATION2 and I @CAPS1 to the @CAPS2 ...   \n",
       " 4198     21915          8   Over this last @DATE1 some friends and I were...   \n",
       " 4199     21916          8     Have you ever had a really bad day and noth...   \n",
       " 4200     21917          8                                  LaughterLaught...   \n",
       " 4201     21918          8  Laughter is a tool that every one knows how to...   \n",
       " 4202     21919          8   Laughter is a good part of life. It demonstra...   \n",
       " 4203     21921          8  It all started off on a cold sunny @DATE1 @TIM...   \n",
       " 4204     21922          8  There have been many times in my life that I h...   \n",
       " 4205     21923          8   The sunset shimmers off of the sparkling wate...   \n",
       " 4206     21924          8    Laughter has played a significant role in my...   \n",
       " 4207     21925          8   In the @DATE1 ,my best friend @PERSON1 and I ...   \n",
       " 4208     21926          8    My friends and I always have a great time to...   \n",
       " 4209     21927          8    Everybody laughs in their life time, sometim...   \n",
       " 4210     21928          8   Laughter to me is important in any relationsh...   \n",
       " 4211     21929          8   I believe that laughter is a huge part of any...   \n",
       " 4212     21932          8                         laughter is the best me...   \n",
       " 4213     21933          8   Have you ever noticed that if two little kids...   \n",
       " 4214     21934          8                              Laughter @CAPS1 I ...   \n",
       " 4215     21935          8   Laughter in @CAPS1 A laugh is not just an act...   \n",
       " 4216     21937          8    LAUGHTER @CAPS1 i was younger my friend live...   \n",
       " 4217     21938          8   You know how the saying goes live, laugh, lov...   \n",
       " \n",
       "       score  unique_words  \n",
       " 0         7      0.604348  \n",
       " 1         8      0.551613  \n",
       " 2         9      0.476190  \n",
       " 3         9      0.443213  \n",
       " 4         9      0.479029  \n",
       " 5         9      0.510870  \n",
       " 6         9      0.478842  \n",
       " 7        11      0.524324  \n",
       " 8         8      0.532203  \n",
       " 9        10      0.474729  \n",
       " 10        8      0.540698  \n",
       " 11        9      0.540230  \n",
       " 12        8      0.476027  \n",
       " 13       12      0.564050  \n",
       " 14        7      0.588235  \n",
       " 15        8      0.498795  \n",
       " 16        8      0.591912  \n",
       " 17        9      0.479310  \n",
       " 18        8      0.576471  \n",
       " 19        8      0.472393  \n",
       " 20        9      0.536649  \n",
       " 21        9      0.528846  \n",
       " 22       11      0.503386  \n",
       " 23       10      0.497345  \n",
       " 24        9      0.489177  \n",
       " 25        8      0.458861  \n",
       " 26        8      0.620321  \n",
       " 27        8      0.535248  \n",
       " 28        9      0.518519  \n",
       " 29        8      0.568027  \n",
       " ...     ...           ...  \n",
       " 4188     40      0.508043  \n",
       " 4189     41      0.480000  \n",
       " 4190     32      0.628763  \n",
       " 4191     41      0.450687  \n",
       " 4192     38      0.404508  \n",
       " 4193     36      0.460199  \n",
       " 4194     44      0.540897  \n",
       " 4195     23      0.601266  \n",
       " 4196     37      0.558511  \n",
       " 4197     39      0.448276  \n",
       " 4198     30      0.650980  \n",
       " 4199     36      0.413018  \n",
       " 4200     27      0.787500  \n",
       " 4201     35      0.420000  \n",
       " 4202     37      0.457967  \n",
       " 4203     39      0.439542  \n",
       " 4204     41      0.415274  \n",
       " 4205     41      0.504122  \n",
       " 4206     39      0.491607  \n",
       " 4207     37      0.407095  \n",
       " 4208     36      0.496815  \n",
       " 4209     35      0.349353  \n",
       " 4210     37      0.417910  \n",
       " 4211     38      0.404402  \n",
       " 4212     33      0.452135  \n",
       " 4213     33      0.460595  \n",
       " 4214     35      0.445070  \n",
       " 4215     38      0.535390  \n",
       " 4216     32      0.428319  \n",
       " 4217     39      0.379019  \n",
       " \n",
       " [4218 rows x 5 columns])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_unique_words_column(train_df, valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing validation data!\n"
     ]
    }
   ],
   "source": [
    "# Show nothing is empty in validation set\n",
    "if valid_df.isnull().any().any():\n",
    "    print ('Validation data is missing!')\n",
    "else:\n",
    "    print ('No missing validation data!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returned a copy of old_df, with essays cleaned for count vectorizer\n",
    "# cleaning returns essay with only lowercase words separated by space\n",
    "def vectorizer_clean(old_df):\n",
    "    new_df = old_df.copy()\n",
    "    for i in range(new_df.shape[0]):\n",
    "        new_df.set_value(i, 'essay', \" \".join(re.sub('[^a-zA-Z\\d\\s]', '', new_df['essay'].iloc[i]).lower().split())) \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   essay_id  essay_set                                              essay  \\\n",
      "0         1          1  dear local newspaper i think effects computers...   \n",
      "1         2          1  dear caps1 caps2 i believe that using computer...   \n",
      "2         3          1  dear caps1 caps2 caps3 more and more people us...   \n",
      "3         4          1  dear local newspaper caps1 i have found that m...   \n",
      "4         5          1  dear location1 i know having computers has a p...   \n",
      "\n",
      "   score  std_score  unique_words  \n",
      "0      8  -0.343483      0.544379  \n",
      "1      9   0.306655      0.515513  \n",
      "2      7  -0.993622      0.598566  \n",
      "3     10   0.956794      0.524809  \n",
      "4      8  -0.343483      0.486022  \n"
     ]
    }
   ],
   "source": [
    "# print essays cleaned for vectorizer (essay is now just lowercase words separated by space) \n",
    "vectorizer_train = vectorizer_clean(train_df)\n",
    "print (vectorizer_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   essay_id  essay_set                                              essay  \\\n",
      "0      1788          1  dear organization1 caps1 more and more people ...   \n",
      "1      1789          1  dear location1 time caps1 me tell you what i t...   \n",
      "2      1790          1  dear local newspaper have you been spending a ...   \n",
      "3      1791          1  dear readers caps1 you imagine how life would ...   \n",
      "4      1792          1  dear newspaper i strongly believe that compute...   \n",
      "\n",
      "   score  unique_words  \n",
      "0      7      0.604348  \n",
      "1      8      0.551613  \n",
      "2      9      0.476190  \n",
      "3      9      0.443213  \n",
      "4      9      0.479029  \n"
     ]
    }
   ],
   "source": [
    "# print essays cleaned for vectorizer (essay is now just lowercase words separated by space) \n",
    "vectorizer_valid = vectorizer_clean(valid_df)\n",
    "print (vectorizer_valid.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "\n",
    "vectorizer2 = TfidfVectorizer(stop_words = 'english', ngram_range=(2,2))\n",
    "vectorizer3 = TfidfVectorizer(stop_words = 'english', ngram_range=(3,3))\n",
    "vectorizer4 = TfidfVectorizer(stop_words = 'english', ngram_range=(4,4))\n",
    "vectorizer5 = TfidfVectorizer(stop_words = 'english', ngram_range=(5,5))\n",
    "\n",
    "\n",
    "#Get all the text from data\n",
    "train_essays = vectorizer_train['essay'].values\n",
    "\n",
    "#Turn each text into an array of word counts\n",
    "train_vectors = vectorizer.fit_transform(train_essays).toarray()\n",
    "\n",
    "train_vectors2 = vectorizer2.fit_transform(train_essays).toarray()\n",
    "train_vectors3 = vectorizer3.fit_transform(train_essays).toarray()\n",
    "train_vectors4 = vectorizer4.fit_transform(train_essays).toarray()\n",
    "train_vectors5 = vectorizer5.fit_transform(train_essays).toarray()\n",
    "\n",
    "\n",
    "#normalizing for y\n",
    "train_std_scores = np.asarray(vectorizer_train['std_score'], dtype=\"byte\")\n",
    "print (train_std_scores[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1296: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "## TfidfVectorizer with ngram=(1,1) ##\n",
    "######################################\n",
    "\n",
    "\n",
    "###############\n",
    "# Logistic L2 #\n",
    "###############\n",
    "\n",
    "# Logistic Model with L2 penalty\n",
    "logistic_l2 = LogReg(penalty='l2', solver='liblinear', n_jobs=4)\n",
    "logistic_l2.fit(train_vectors, train_std_scores)\n",
    "\n",
    "valid_vectors = vectorizer.transform(vectorizer_valid['essay'].values).toarray()\n",
    "\n",
    "# My guess is we will want to denormalize these scores for quadratic weighted k\n",
    "valid_pred_std_scores_l2 = logistic_l2.predict(valid_vectors)\n",
    "\n",
    "# Appending predicted scores to validation data set\n",
    "valid_df[\"Log_L2 predicted_scores\"] = valid_pred_std_scores_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "essay_id                           int64\n",
       "essay_set                          int64\n",
       "essay                             object\n",
       "score                              int64\n",
       "unique_words                     float64\n",
       "Log_L2 predicted_scores             int8\n",
       "newly_predicted_scores_log_l2      int64\n",
       "Log_L1 predicted_scores              |S6\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#denormalizing the values and placing them into the stand_pred_values array\n",
    "stand_pred_values_l2 = []\n",
    "for i in range(max_essay_set):\n",
    "    current_set = valid_df[valid_df['essay_set'] == i + 1]['Log_L2 predicted_scores']\n",
    "    for value in current_set:\n",
    "        stand_pred_values_l2.append(int(float(value) * float(regularization_data[i][2]) + (regularization_data[i][1])))\n",
    "# print stand_pred_values_l2\n",
    "\n",
    "#adding the denormalizede predicted values to the valid_df dataset\n",
    "valid_df['newly_predicted_scores_log_l2'] = stand_pred_values_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1296: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# Logistic L1 #\n",
    "###############\n",
    "\n",
    "# Logistic Model with L1 penalty\n",
    "logistic_l1 = LogReg(penalty='l1', solver='liblinear', n_jobs=4)\n",
    "logistic_l1.fit(train_vectors, train_std_scores)\n",
    "\n",
    "valid_pred_std_scores_l1 = logistic_l1.predict(valid_vectors)\n",
    "\n",
    "\n",
    "# Appending predicted scores to validation data set\n",
    "valid_df['Log_L1 predicted_scores'] = valid_pred_std_scores_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#denormalizing the values and placing them into the stand_pred_values array\n",
    "stand_pred_values_l1 = []\n",
    "for i in range(max_essay_set):\n",
    "    current_set = valid_df[valid_df['essay_set'] == i + 1]['Log_L1 predicted_scores']\n",
    "    for value in current_set:\n",
    "        stand_pred_values_l1.append(int(float(value) * float(regularization_data[i][2]) + (regularization_data[i][1])))\n",
    "# print stand_pred_values_l1\n",
    "\n",
    "#adding the denormalizede predicted values to the valid_df dataset\n",
    "valid_df['newly_predicted_scores_log_l1'] = stand_pred_values_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC L2\n",
      "Number of correct predictions = 1340\n",
      "Total number of observations = 4218\n",
      "Score = 0.3176861071597914\n",
      "\n",
      "LOGISTIC L1\n",
      "Number of correct predictions = 1346\n",
      "Total number of observations = 4218\n",
      "Score = 0.319108582266477\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "#   Scoring   #\n",
    "###############\n",
    "\n",
    "#Scoring the predicted values with the actual values\n",
    "log_l2_count = 0\n",
    "log_l1_count = 0\n",
    "for i in range(len(valid_df)):\n",
    "    if valid_df.iloc[i]['score'] == valid_df.iloc[i]['newly_predicted_scores_log_l2']:\n",
    "        log_l2_count += 1\n",
    "    if valid_df.iloc[i]['score'] == valid_df.iloc[i]['newly_predicted_scores_log_l1']:\n",
    "        log_l1_count += 1\n",
    "        \n",
    "print (\"LOGISTIC L2\")\n",
    "print (\"Number of correct predictions =\", log_l2_count)\n",
    "print (\"Total number of observations =\", len(valid_df))\n",
    "print (\"Score =\", float(log_l2_count) / len(valid_df))\n",
    "\n",
    "print (\"\")\n",
    "print (\"LOGISTIC L1\")\n",
    "print (\"Number of correct predictions =\", log_l1_count)\n",
    "print (\"Total number of observations =\", len(valid_df))\n",
    "print (\"Score =\", float(log_l1_count) / len(valid_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic L2: SpearmanrResult(correlation=0.9199667290616039, pvalue=0.0)\n",
      "Logistic L1: SpearmanrResult(correlation=0.9176427687752943, pvalue=0.0)\n"
     ]
    }
   ],
   "source": [
    "#Spearman Correlation Coefficient\n",
    "from scipy.stats import spearmanr as Spearman\n",
    "\n",
    "print (\"Logistic L2:\", Spearman(a = valid_df[\"score\"], b = valid_df[\"newly_predicted_scores_log_l2\"]))\n",
    "print (\"Logistic L1:\", Spearman(a = valid_df[\"score\"], b = valid_df[\"newly_predicted_scores_log_l1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, as we expand ngram length to 2, we see that the computation power required becomes such that the kernel dies.  Therefore, we will have to try limiting the number of words included in the vectorizer for our future models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "## TfidfVectorizer with ngram=(2,2) ##\n",
    "######################################\n",
    "\n",
    "###############\n",
    "# Logistic L2 #\n",
    "###############\n",
    "\n",
    "# Logistic Model with L2 penalty\n",
    "logistic_l2 = LogReg(penalty='l2', solver='liblinear', n_jobs=4)\n",
    "logistic_l2.fit(train_vectors2, train_std_scores)\n",
    "\n",
    "valid_vectors2 = vectorizer2.transform(vectorizer_valid['essay'].values).toarray()\n",
    "\n",
    "# My guess is we will want to denormalize these scores for quadratic weighted k\n",
    "valid_pred_std_scores_l2 = logistic_l2.predict(valid_vectors2)\n",
    "\n",
    "# Appending predicted scores to validation data set\n",
    "valid_df[\"Log_L2 predicted_scores_2\"] = valid_pred_std_scores_l2\n",
    "\n",
    "#denormalizing the values and placing them into the stand_pred_values array\n",
    "stand_pred_values_l2 = []\n",
    "for i in range(max_essay_set):\n",
    "    current_set = valid_df[valid_df['essay_set'] == i + 1]['Log_L2 predicted_scores_2']\n",
    "    for value in current_set:\n",
    "        stand_pred_values_l2.append(int(float(value) * float(regularization_data[i][2]) + (regularization_data[i][1])))\n",
    "# print stand_pred_values_l2\n",
    "\n",
    "#adding the denormalizede predicted values to the valid_df dataset\n",
    "valid_df['newly_predicted_scores_log_l2_2'] = stand_pred_values_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############\n",
    "# Logistic L1 #\n",
    "###############\n",
    "\n",
    "# Logistic Model with L1 penalty\n",
    "logistic_l1 = LogReg(penalty='l1', solver='liblinear', n_jobs=4)\n",
    "logistic_l1.fit(train_vectors2, train_std_scores)\n",
    "\n",
    "valid_pred_std_scores_l1 = logistic_l1.predict(valid_vectors2)\n",
    "\n",
    "\n",
    "# Appending predicted scores to validation data set\n",
    "valid_df['Log_L1 predicted_scores_2'] = valid_pred_std_scores_l1\n",
    "\n",
    "#denormalizing the values and placing them into the stand_pred_values array\n",
    "stand_pred_values_l1 = []\n",
    "for i in range(max_essay_set):\n",
    "    current_set = valid_df[valid_df['essay_set'] == i + 1]['Log_L1 predicted_scores_2']\n",
    "    for value in current_set:\n",
    "        stand_pred_values_l1.append(int(float(value) * float(regularization_data[i][2]) + (regularization_data[i][1])))\n",
    "# print stand_pred_values_l1\n",
    "\n",
    "#adding the denormalizede predicted values to the valid_df dataset\n",
    "valid_df['newly_predicted_scores_log_l1_2'] = stand_pred_values_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-09640cbbc1cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlog_l2_count_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlog_l1_count_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalid_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mvalid_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'newly_predicted_scores_log_l2_2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mlog_l2_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valid_df' is not defined"
     ]
    }
   ],
   "source": [
    "###############\n",
    "#   Scoring   #\n",
    "###############\n",
    "\n",
    "#Scoring the predicted values with the actual values\n",
    "log_l2_count_2 = 0\n",
    "log_l1_count_2 = 0\n",
    "for i in range(len(valid_df)):\n",
    "    if valid_df.iloc[i]['score'] == valid_df.iloc[i]['newly_predicted_scores_log_l2_2']:\n",
    "        log_l2_count += 1\n",
    "    if valid_df.iloc[i]['score'] == valid_df.iloc[i]['newly_predicted_scores_log_l1_2']:\n",
    "        log_l1_count += 1\n",
    "        \n",
    "print (\"LOGISTIC L2\")\n",
    "print (\"Number of correct predictions =\", log_l2_count_2)\n",
    "print (\"Total number of observations =\", len(valid_df))\n",
    "print (\"Score =\", float(log_l2_count_2) / len(valid_df))\n",
    "\n",
    "print (\"\")\n",
    "print (\"LOGISTIC L1\")\n",
    "print (\"Number of correct predictions =\", log_l1_count_2)\n",
    "print (\"Total number of observations =\", len(valid_df))\n",
    "print (\"Score =\", float(log_l1_count_2) / len(valid_df))\n",
    "\n",
    "#Spearman Correlation Coefficient\n",
    "from scipy.stats import spearmanr as Spearman\n",
    "\n",
    "print (\"Logistic L2:\", Spearman(a = valid_df[\"score\"], b = valid_df[\"newly_predicted_scores_log_l2_2\"]))\n",
    "print (\"Logistic L1:\", Spearman(a = valid_df[\"score\"], b = valid_df[\"newly_predicted_scores_log_l1_2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
